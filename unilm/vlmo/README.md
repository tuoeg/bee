# VLMo - General-purpose Multimodal Pre-training

**[VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://arxiv.org/abs/2111.02358). Wenhui Wang, Hangbo Bao, Li Dong, Furu Wei. arXiv 2021.**

Official PyTorch implementation and pretrained models of VLMo.

- May 30th, 2022: new version of [**VLMo** paper on arXiv](https://arxiv.org/pdf/2111.02358.pdf).
- November 24th, 2021: **VLMo** Large (**single** model) as the new SOTA on the [VQA Challenge](https://eval.ai/web/challenges/challenge-page/830/leaderboard/2278)
- Nov 2021: release preprint in [arXiv](https://arxiv.org/abs/2111.02358)


## Citation

If you find this repository useful, please consider citing our work:
```
@article{vlmo,
      title={{VLMo}: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts}, 
      author={Wenhui Wang and Hangbo Bao and Li Dong and Furu Wei},
      year={2021},
      eprint={2111.02358},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```


## License
This project is licensed under the license found in the LICENSE file in the root directory of this source tree.

[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct)

### Contact Information

For help or issues using VLMo models, please submit a GitHub issue.

For other communications related to UniLM AI, please contact Li Dong (`lidong1@microsoft.com`), [Furu Wei](http://gitnlp.org/) (`fuwei@microsoft.com`).
